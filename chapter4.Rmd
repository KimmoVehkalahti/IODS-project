# Week 4: Clustering and Classification

The topics of this chapter - clustering and classification - are handy and visual tools of exploring statistical data. Clustering means that some points (or observations) of the data are in some sense closer to each other than some other points. In other words, the data points do not comprise a homogeneous sample, but instead, it is somehow clustered.

In general, the clustering methods try to find these clusters (or groups) from the data. One of the most typical clustering methods is called k-means clustering. Also hierarchical clustering methods quite popular, giving tree-like dendrograms as their main output.

As such, clusters are easy to find, but what might be the "right" number of clusters? It is not always clear. And how to give these clusters names and interpretations?

Based on a successful clustering, we may try to classify new observations to these clusters and hence validate the results of clustering. Another way is to use various forms of discriminant analysis, which operates with the (now) known clusters, asking: "what makes the difference(s) between these groups (clusters)?"

In the connection of these methods, we also discuss the topic of distance (or dissimilarity or similarity) measures. There are lots of other measures than just the ordinary Euclidean distance, although it is one of the most important ones. Several discrete and even binary measures exist and are widely used for different purposes in various disciplines.

## 4.0 Packages for clustering and classification

```{r}
library(tidyverse)
library(GGally)
library(dplyr)
library(ggplot2)
library(MASS)
library(corrplot)
```

## 4.1 loading data and Exploring the data

```{r}
#Loading the Boston data from the MASS package
data("Boston")

# Exploring the structure and the dimensions of the data set

str(Boston)
dim(Boston)
```
The data set "Boston" from MASS library presents geographical, demographic, structural, economic and cultural description of different suburbs in Boston and their implication on housing values within different suburbs. Data set contains 14 variables (factors) and 506 observation and most variables are numerical.

The variables in the data set which influences the housing prices are: 

crim = per capita crime rate by town.
zn = proportion of residential land zoned for lots over 25,000 sq.ft.
indus = proportion of non-retail business acres per town.
chas = Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
nox = nitrogen oxides concentration (parts per 10 million).
rm = average number of rooms per dwelling.
age = proportion of owner-occupied units built prior to 1940.
dis = weighted mean of distances to five Boston employment centres.
rad = index of accessibility to radial highways.
tax = full-value property-tax rate per $10,000.
ptratio = pupil-teacher ratio by town.
black = 1000(Bk - 0.63)^2 where BkBk is the proportion of blacks by town.
lstat = lower status of the population (percent).
medv = median value of owner-occupied homes in $1000s.

Data is sourced from 
Harrison, D. and Rubinfeld, D.L. (1978) Hedonic prices and the demand for clean air. J. Environ. Economics and Management 5, 81â€“102.
Belsley D.A., Kuh, E. and Welsch, R.E. (1980) Regression Diagnostics. Identifying Influential Data and Sources of Collinearity. New York: Wiley.

For more information about the "Boston" data set follow the link https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html

## 4.2 Graphical Overview of the data set

```{r}
#overview
summary(Boston)

#Plotting 
long_Boston <- pivot_longer(Boston, cols=everything(), names_to = 'variable', values_to = 'value')
p1 <- ggplot(data=long_Boston)
p1 + geom_histogram(mapping= aes(x=value)) + facet_wrap(~variable, scales="free")
```



```{r}
#Relationship between the variables
cor_matrix <- cor(Boston)
cor_matrix
corrplot(cor_matrix, method="circle")

#Let's try a mixed plot with correlation values (closer the values to 1, stronger the correlation) 
corrplot.mixed(cor_matrix, lower = 'number', upper = 'circle',tl.pos ="lt", tl.col='black', tl.cex=0.6, number.cex = 0.5)
```
In the correlation matrix, Color blue represents positive and red represents the negative correlation. The values closer to +1 and -1 implies a stronger positive and negative correlation respectively. The threshold is represented by different shades of blue and black and decimal points which is indexed on the right side of the plot. 

Based on the plot we can see:

Positive correlation between some variables for example:

- Proportional of non-retail business acres per town (indus) and nitrogen oxides concentration in ppm (nox)
- Index of accessibility to radial highways (rad) and full-value property-tax rate per $10,000 (tax)

Negative correlation between some variables for example:

- Proportional of non-retail business acres per town (indus) and weighted mean of distances to five Boston employment centers (dis) 
- Nitrogen oxides concentration in ppm (nox) and weighted mean of distances to five Boston employment centers (dis) 

## 4.3 Standardizing the dataset

Lets beging the scaling exercise to standardize the data set. Since the Boston data contains only numerical values, so we can use the function `scale()` to standardize the whole dataset as mentioned in the exercise 4 instruction.

We also have the following equation from exercise which basically gives us the idea on scaling i.e., subtract the column means from corresponding means and dividing with the standard deviation. 

$$scaled(x) = \frac{x - mean(x)}{ sd(x)}$$

```{r}
#Saving the scaled data to the object 
boston_scaled <- scale(Boston)

#Lets see
summary(boston_scaled)

# class of the boston_scaled object
class(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)
```
Scaling the data set with various scales makes the analyses easier. Scale() transforms the data into a matrix and array so for further analysis we can change it back to the data frame

Let's scale further by creating a quantile vector of crim and print it

```{r}
bins <- quantile(boston_scaled$crim)
bins

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE)

# look at the table of the new factor crime
table(crime)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

# let's see the new data set now !! 
summary(boston_scaled)
```

Now as mention in the exercise set "Divide and conquer", lets divide train and test sets: training -> 80% and testing -> 20%

```{r}

# number of rows in the Boston data set 
n <- nrow(boston_scaled)
n

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

```

## 4.4 Fitting the linear discriminant analysis

Let's move on fitting LDA on the training set. 

Our target variable: crime(categorical)

```{r}

# linear discriminant analysis
# Other variables are designated (.)
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit
```

There are three discriminant function LD(proportion of trace) as follow:

   LD1    LD2    LD3 
0.9489 0.0362 0.0150

```{r}
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)
```

## 4.5 Predicting the LDA model

```{r}
library(MASS)

ind <- sample(nrow(boston_scaled),  size = nrow(boston_scaled) * 0.8)

train <- boston_scaled[ind,]

test <- boston_scaled[-ind,]

correct_classes <- test$crime

test <- dplyr::select(test, -crime)

lda.fit = lda(crime ~ ., data=train)

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)

```

## 4.6 Model Optimization and K means clustering

Let's work with clustering now
Let's calculate also the distances between observations to assess the similarity between data points. As instructed we calculated two distance matrix euclidean and manhattan. 

```{r}
#start by reloading the Boston data set
data("Boston")

boston_scaled <- scale(Boston)
summary(boston_scaled)

#class of Boston_scaled object
class(boston_scaled)

# change the object to data frame for futher analysis
boston_scaled <- as.data.frame(boston_scaled)

```
```{r}
# Calculating distances between the observation
# euclidean distance matrix
dist_eu <- dist(boston_scaled, method = "euclidean")

# look at the summary of the distances
summary(dist_eu)

# Manhattan distance matrix
dist_man <- dist(boston_scaled, method = "manhattan")

# look at the summary of the distances
summary(dist_man)
```

Lets do K-means clustering (first with 4 clusters and then 3)

```{r}

set.seed(123) #function set.seed() is used here to deal with the random assigning of the initial cluster centers when conducting k-means clustering

# k-means clustering
km <- kmeans(boston_scaled, centers = 4)

# plotting the scaled Boston data set with clusters
pairs(boston_scaled, col = km$cluster)
```
With 4 clusters, it appears that there is some overlapping between clusters. lets try 3 clusters and check 
```{r}
# k-means clustering
km <- kmeans(boston_scaled, centers = 3)

# plotting the scaled Boston data set with clusters
pairs(boston_scaled, col = km$cluster)
```
with 3 clusters it is even worse !! 
We can try to determine the optimal number of cluster for our model and see how it works

## 4.7 K-means: determine the k

```{r}

#Investigating the optimal number of clusters
set.seed(123) #setseed function again

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line', ylab = "TWCSS", xlab = "Number of cluster")
```

From the plot it looks like the optimal number of clusters for our model is 2-3 as there is a sharp drop in TWCSS values. We can try with 2 because 2 seems more optimal as by 2.5 the drop is significant and we can see the substantial change.

```{r}
# k-means clustering
km <- kmeans(boston_scaled, centers = 2)

# plot the Boston data set with clusters predicted
pairs(boston_scaled, col = km$cluster)
```
```{r}
pairs(boston_scaled[6:10], col = km$cluster)
```
With 2 clusters, the model seem better, there is good separation for example rad and tax. rm and age seems ok as well. 
