## Clustering and classification
<br>

Clustering and classification is the best way to visualize and validate data integrity and groups. One of the best ways to explore study group behaviour and most often also used as a good quality data analysis protocol. 

Some places to dig into this

- [The 5 Methods of clustering](https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68)

- [Cluster Analysis -chapter](https://www-users.cs.umn.edu/~kumar001/dmbook/ch8.pdf)

***
<br>

#### The Data Analysis
<br>
__Load data and describe it (#2)__

The dataset of Boston Housing from package MASS was taken from the StatLib library which is maintained at Carnegie Mellon University. 

More information on the package (page 21-) [MASS](https://cran.r-project.org/web/packages/MASS/MASS.pdf)


The dataframe consist of 506 rows and 14 colums of housing data originally described by [Harrison and Rubinfeld (1979)](https://www.sciencedirect.com/science/article/abs/pii/0095069678900062?via%3Dihub).

The variables are:

- #1 CRIM     – per capita crime rate by town
- #2 ZN     – proportion of residential land zoned for lots over 25,000 sq.ft
- #3 INDUS     – proportion of non-retail business acres per town
- #4 CHAS     – Charles River dummy variable (1 if tract bounds river; else 0)
- #5 NOX     – nitric oxides concentration (parts per 10 million)
- #6 RM     – average number of rooms per dwelling
- #8 AGE     – proportion of owner-occupied units built prior to 1940
- #8 DIS     – weighted distances to five Boston employment centres
- #9 RAD     – index of accessibility to radial highways
- #10 TAX     – full-value property-tax rate per $10,000
- #11 PTRATIO     – pupil-teacher ratio by town
- #12 B     – 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
- #13 LSTAT     – % lower status of the population
- #14 MEDV     – Median value of owner-occupied homes in $1000’s

```{r libraries4a, results='hide',message=FALSE}
library(MASS)
library(tidyr)
library(dplyr)
library(GGally)
library(ggpubr)
library(corrplot)
library(skimr)
library(psych)
```
<br>
__Graphical and numerical explorations of data (#3)__
```{r}
data("Boston")
class(Boston)
dim(Boston)
str(Boston)
summary(Boston)
boxplot(Boston)
cor_matrix<-cor(Boston) %>% round(digits = 2)
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex = 0.6)
```


COMMENTS: 


<br>
__Standardize and scale (#4)__

__*Centering data with scale -*function*__
```{r}
# center and standardize variables
boston_scaled <- scale(Boston)
# summaries of the scaled variables
summary(boston_scaled)
# class of the boston_scaled object
class(boston_scaled)
# change the object to data frame and verify class
boston_scaled<-as.data.frame(boston_scaled)
class(boston_scaled)
```

COMMENTS: All The mean values are zero and the variance has been mimnimized.   
<br>
__*creating a categorical variable from crim to crime using quantiles as break points and dividing dataset to training and testing sets (20% and 80% respectively).*__ 
```{r}
# summary of the scaled crime rate
summary(boston_scaled$crim)
# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins
# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))
# look at the table of the new factor crime
table(crime)
# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)
# add the new categorical value (crime) to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
```

```{r}
# number of rows in the Boston dataset 
n <- nrow(boston_scaled)
# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)
# create train set
train <- boston_scaled[ind,]
head(train)
table(crime)
# create test set 
test <- boston_scaled[-ind,]
head(test)
# save the correct classes from test data
correct_classes <- test$crime
# remove the crime variable from test data
test <- dplyr::select(test, -crime)
head(test)
```
<br>

__Linear Discriminant Analysis (#5, #6)__ 

```{r}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
# target classes as numeric ("the step of saving categories and removing categorical variable")
classes <- as.numeric(train$crime)
# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)
lda.pred <- predict(lda.fit, newdata = test)
table(correct = correct_classes, predicted = lda.pred$class)

```

COMMENTS: High crime rate showns up on it's own cluster. 

<br>
__K-means Analysis #7__

- Reload Boston dataset and standardize for calculating distances between observations

```{r}

data("Boston")
class(Boston)
dim(Boston)
# center and standardize variables
boston_scaled <- scale(Boston)
# summaries of the scaled variables
class(boston_scaled)
# change the object to data frame and verify class
boston_scaled<-as.data.frame(boston_scaled)
class(boston_scaled)
# euclidean distance matrix
dist_eu <- dist(boston_scaled)
# manhattan distance matrix
dist_man <- dist(boston_scaled, method = 'manhattan')
# k-means clustering
km <-kmeans(boston_scaled, centers = 4)
# plot the Boston dataset with clusters
pairs(boston_scaled, col = km$cluster)
# determine the number of clusters
k_max <- 8
# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})
# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')
# k-means clustering
km <-kmeans(boston_scaled, centers = 4)

# plot the Boston dataset with clusters
pairs(boston_scaled, col = km$cluster)

```



COMMENTS: Based on several runs and the visual characters 4 clusters should be the maximum. Dispite the distance gradually decreases with increased number of clusters there is no need to increase the number above 3-5 since their common charasteristics are the main goal. 
<br>

__Bonuses__

```{r}

data("Boston")
class(Boston)
dim(Boston)
# k-means clustering using 4 clusters
km <-kmeans(Boston, centers = 4)


# center and standardize variables
boston_scaled <- scale(Boston)
class(boston_scaled)
# change the object to data frame and verify class
boston_scaled<-as.data.frame(boston_scaled)
class(boston_scaled)



# euclidean distance matrix
dist_eu <- dist(boston_scaled)
# manhattan distance matrix
dist_man <- dist(boston_scaled, method = 'manhattan')

# plot the Boston dataset with clusters
pairs(boston_scaled, col = km$cluster)
# determine the number of clusters
k_max <- 8
# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})
# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')
# k-means clustering
km <-kmeans(boston_scaled, centers = 4)

# plot the Boston dataset with clusters
pairs(boston_scaled, col = km$cluster)

```

COMMENTS:



<br>
#### The Data Wrangling 

Using libraries same as previously


__Loading data and explorations (#1-3)__

```{r}
hd <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human_development.csv", stringsAsFactors = F)

gii <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/gender_inequality.csv", stringsAsFactors = F, na.strings = "..")

#creating summaries of each dataset
class(hd)
dim(hd)
head(hd)
str(hd)
summary(hd)
colnames(hd)

class(hd)
dim(gii)
head(gii)
str(gii)
summary(gii)
colnames(gii)
```

<br>

__Renaming the variables(#4)__ 

```{r}
colnames(hd)
colnames(gii)
hd = rename(hd, "hdi_rank" = "HDI.Rank", "country" = "Country", "hdi" = "Human.Development.Index..HDI.", "Life.Exp"  = "Life.Expectancy.at.Birth","Edu.Exp" =  "Expected.Years.of.Education" , "edu_mean" = "Mean.Years.of.Education" , "GNI" = "Gross.National.Income..GNI..per.Capita", "gnirank_hdirank" = "GNI.per.Capita.Rank.Minus.HDI.Rank")

gii = rename(gii, "gii_rank" = "GII.Rank", "country" = "Country" , "gii" = "Gender.Inequality.Index..GII." , "Mat.Mor"  = "Maternal.Mortality.Ratio", "Ado.Birth" =  "Adolescent.Birth.Rate" , "Parli.F" = "Percent.Representation.in.Parliament" , "Edu2.F" = "Population.with.Secondary.Education..Female.", "Edu2.M" = "Population.with.Secondary.Education..Male.", "Labo.F" = "Labour.Force.Participation.Rate..Female.", "Labo.M" = "Labour.Force.Participation.Rate..Male.")

#Also modified the data acc DataCamp
str(hd$gni)
#str_replace(hd$gni, pattern=",", replace ="") %>% as.numeric
```


```{r}
colnames(hd)
colnames(gii)
```

__Mutating the Gender inequality” data (#5)__

```{r}

mutate(gii, Edu2.FM = Edu2.F / Edu2.M)
mutate(gii, Labo2.FM = Labo.F / Labo.M)

colnames(hd)
colnames(gii)
```

__InnerJoin the files(#6)__

```{r}
join_by <- c("country")
human <- inner_join(hd, gii, by = join_by, suffix = c("hd", "gii"))
str(human)
```







***


<br>

 

***
