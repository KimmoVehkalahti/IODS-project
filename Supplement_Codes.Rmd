---
title: "supplement codes"
author: "Rong Guang"
date: '2022-11-11'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


### read the data set

```{r}
library(tidyverse)
learn <- read_csv(file = "data/learning2014.csv")
```

### Code categorical data

```{r}
learn <- learn %>% 
  mutate(gender = gender %>%
           factor() %>% 
           fct_recode("Female" = "F",
                      "Male" = "M"))
```

### generate cooks' distance for each obs

```{r}
fit2 <- learn %>% 
  lm(points ~ attitude  + poly(age, 2, raw =T), data = .)
summary(fit2)
```

```{r}
cooksd <- cooks.distance(fit2)
cooksd <- data.frame(index = 1:length(cooksd), cooksd = cooksd)
```

### plot cooks's distance against index number

```{r}
plot(cooksd$index, 
     cooksd$cooksd, 
     pch="*", cex=1.5, 
     main="Influential Obs by Cooks distance") +
abline(h = 4*mean(cooksd$cooksd), 
       col="red")+
text(x=cooksd$index+4, 
     y=cooksd$cooksd, 
     labels= ifelse(cooksd$cooksd>4*mean(cooksd$cooksd, na.rm=T),
                    cooksd$index,""),
     col="red")


```

### generate an index for original data set and merge cooks's distance into it 

```{r}
learn <- learn %>% mutate(index = 1:nrow(learn))
learn <- left_join(learn, cooksd, by = "index")
```

### check the variables with large influence

```{r}
learn %>% filter(index %in% c(1,4,3,10,19,24,35,56,145)) %>% arrange(desc(cooksd))
```

###check the varible with large influence to residual normality and linearity

```{r}
plot(fit2, which = c(1,2))
```

It is found that the data points 145, 35, and 56 have large influence on residuals' normality and linearity as well as the predictor's estimated coefficient. I will remove these observations and fit the model again from a data-drive perspective.

```{r}
fit3 <- learn %>% 
  filter(!index %in% c(56, 145, 35)) %>% 
  lm(points ~ attitude  + poly(age, 2, raw =T), data = .)
summary(fit3)
```

The new model with influential cases removed showed significant coefficient estimates for each predictor, as the previous model. More notably, the adjusted R-squared has increased to 0.303, indicating the model would explain 30.3% of variability of exam points. This is a remarkable increase from the previous model, which had a adjusted R-squared of 0.23. 


```{r}
#Codes not in use
#learn <- learn %>% mutate(age.factor = age %>% 
#                            cut(breaks =c(0,19,22,25,27,34,100)))
```


