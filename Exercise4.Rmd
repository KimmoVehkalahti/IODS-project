---
title: "**Introduction to Open Data Science, Exercise Set 4**"

subtitle: "**Clustering and classification**"

output: 
  html_document:
    theme: flatly
    highlight: haddock
    toc: true
    toc_depth: 2
    number_section: false
---


This set consists of a few numbered exercises.
Go to each exercise in turn and do as follows:

1. Read the brief description of the exercise.
2. Run the (possible) pre-exercise-code chunk.
3. Follow the instructions to fix the R code!

## 4.0 INSTALL THE REQUIRED PACKAGES FIRST!

One or more extra packages (in addition to `tidyverse`) will be needed below.

```{r}
# Select (with mouse or arrow keys) the install.packages("...") and
# run it (by Ctrl+Enter / Cmd+Enter):

# install.packages(c("MASS", "corrplot"))
```


## 4.1 Datasets inside R  

R has many (usually small) datasets already loaded in. There are also datasets included in the package installations. Some of the datasets are quite famous (like the [Iris](https://en.wikipedia.org/wiki/Iris_flower_data_set) flower data) and they are frequently used for teaching purposes or to demonstrate statistical methods. 

We will be using the [Boston](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html) dataset from the MASS package. Let's see how it looks like!

```{r, echo=FALSE}
# Pre-exercise-code (Run this code chunk first! Do NOT edit it.)

# Click the green arrow ("Run Current Chunk") in the upper-right corner of this chunk. This will initialize the R objects needed in the exercise. Then move to Instructions of the exercise to start working.

# (no pre-code in this exercise!)
```

### Instructions
- Load the `Boston` dataset from MASS
- Explore the `Boston` dataset. Look at the structure with `str()` and use `summary()` to see the details of the variables.
- Draw the plot matrix with `pairs()`

Hint:
- You can draw the `pairs()` plot by typing the object name where it is saved.

### R code
```{r}
# This is a code chunk in RStudio editor.
# access the MASS package
library(MASS)

# load the data
data("Boston")

# explore the dataset
str(Boston)
library(finalfit)
library(DT)
library(tidyverse)
library(Hmisc)
summary(Boston)
ff_glimpse(Boston)
describe(Boston)
# plot matrix of the variables
pairs(Boston)
```


## 4.2 Correlations plot

It is often interesting to look at the correlations between variables in the data. The function `cor()` can be used to create the correlation matrix. A more visual way to look at the correlations is to use `corrplot()` function (from the corrplot package).

Use the corrplot to visualize the correlation between variables of the Boston dataset.

**Note:** You should first install the package `corrplot`.

```{r, echo=FALSE}
# Pre-exercise-code (Run this code chunk first! Do NOT edit it.)

# Click the green arrow ("Run Current Chunk") in the upper-right corner of this chunk. This will initialize the R objects needed in the exercise. Then move to Instructions of the exercise to start working.

library(MASS)
library(tidyr)
library(corrplot)
data("Boston")

```

### Instructions

- Calculate the correlation matrix and save it as `cor_matrix`. Print the matrix to see how it looks like.
- Adjust the code: use the pipe (`%>%`) to round the matrix. Rounding can be done with the `round()` function. Use the first two digits. Print the matrix again. 
- Plot the rounded correlation matrix
- Adjust the code: add argument `type = "upper"` to the plot. Print the plot again. 
- Adjust the code little more: add arguments `cl.pos = "b"`, `tl.pos = "d"` and `tl.cex = 0.6` to the plot. Print the plot again. 
- See more of corrplot [here](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html)

Hints:
- For correlation matrices, see `?cor`
- The pipe (`%>%`) takes the data before the pipe and applies the functionality assigned in the right. For example `data_column %>% summary()` creates a summary of the data_column.

### R code
```{r}
# Work with the exercise in this chunk, step-by-step. Fix the R code!
# Boston dataset is available

# calculate the correlation matrix and round it
cor_matrix <- cor(Boston) 

# print the correlation matrix
cor_matrix
# visualize the correlation matrix
library(corrplot)
corrplot(cor_matrix, method="circle")

```


## 4.3 Scale the whole dataset

Usually the R datasets do not need much data wrangling as they are already in a good shape. But we will need to do little adjustments.

For later use, we will need to scale the data. In the scaling we subtract the column means from the corresponding columns and divide the difference with standard deviation.

$$scaled(x) = \frac{x - mean(x)}{ sd(x)}$$

The Boston data contains only numerical values, so we can use the function `scale()` to standardize the whole dataset.

```{r, echo=FALSE}
# Pre-exercise-code (Run this code chunk first! Do NOT edit it.)

# Click the green arrow ("Run Current Chunk") in the upper-right corner of this chunk. This will initialize the R objects needed in the exercise. Then move to Instructions of the exercise to start working.

library(MASS)
data("Boston")
```

### Instructions
- Use the `scale()` function on the `Boston` dataset. Save the scaled data to `boston_scaled` object.
- Use `summary()` to look at the scaled variables. Note the means of the variables.
- Find out the class of the scaled object by executing the `class()` function. 
- Later we will want the data to be a data frame. Use `as.data.frame()` to convert the `boston_scaled` to a data frame format. Keep the object name as `boston_scaled`.

Hint:
- See `?scale`

### R code
```{r}
# Work with the exercise in this chunk, step-by-step. Fix the R code!
# Boston dataset is available

# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# class of the boston_scaled object
class(boston_scaled)
boston_scaled
# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)
boston_scaled
```


## 4.4 Creating a factor variable

We can create a categorical variable from a continuous one. There are many ways to to do that. Let's choose the variable `crim` (per capita crime rate by town) to be our factor variable. We want to cut the variable by [quantiles](https://en.wikipedia.org/wiki/Quantile) to get the high, low and middle rates of crime into their own categories.

See how it's done below!

```{r, echo=FALSE}
# Pre-exercise-code (Run this code chunk first! Do NOT edit it.)

# Click the green arrow ("Run Current Chunk") in the upper-right corner of this chunk. This will initialize the R objects needed in the exercise. Then move to Instructions of the exercise to start working.

library(MASS)
data("Boston")
boston_scaled <- as.data.frame(scale(Boston))
boston_scaled$crim <- as.numeric(boston_scaled$crim)
```

### Instructions
- Look at the summary of the scaled variable `crim`
- Use the function `quantile()` on the scaled crime rate variable and save the results to `bins`. Print the results.
- Create categorical crime vector with the `cut()` function. Set the `breaks` argument to be the quantile vector you just created.
- Use the function `table()` on the `crime` object
- Adjust the code of `cut()` by adding the `label` argument in the function. Create a string vector with the values `"low"`, `"med_low"`, `"med_high"`, `"high"` (in that order) and use it to set the labels.
- Do the table of the `crime` object again
- Execute the last lines of code to remove the original crime rate variable and adding the new one to scaled Boston dataset.

Hints:
- You can create a vector with `c()`
- Separate the values with comma in a vector
- Remember that strings need the quotes around them
- `table(*object_name*)` creates a cross tabulation of the object

### R code
```{r}
# Work with the exercise in this chunk, step-by-step. Fix the R code!
# Boston and boston_scaled are available
# summary of the scaled crime rate
summary(boston_scaled$crim)

# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE)
boston_scaled$crime <- crime
# look at the table of the new factor crime
table(boston_scaled$crime)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)


```


## 4.5 Divide and conquer: train and test sets

When we want to use a statistical method to predict something, it is important to have data to test how well the predictions fit. Splitting the original data to test and train sets allows us to check how well our model works. 

The training of the model is done with the train set and prediction on new data is done with the test set. This way you have true classes / labels for the test data, and you can calculate how well the model performed in prediction. 
 
Time to split our data!

```{r, echo=FALSE}
# Pre-exercise-code (Run this code chunk first! Do NOT edit it.)

# Click the green arrow ("Run Current Chunk") in the upper-right corner of this chunk. This will initialize the R objects needed in the exercise. Then move to Instructions of the exercise to start working.

library(dplyr)
library(MASS)
boston_scaled <- read.table("https://raw.githubusercontent.com/KimmoVehkalahti/Helsinki-Open-Data-Science/master/datasets/boston_scaled.txt",
                            sep=",", header = T)
```

### Instructions
- Use the function `nrow()` on the `boston_scaled` to get the number of rows in the dataset. Save the number of rows in `n`.
- Execute the code to choose randomly 80% of the rows and save the row numbers to `ind`
- Create `train` set by selecting the row numbers that are saved in `ind`.
- Create `test` set by subtracting the rows that are used in the train set
- Take the crime classes from the `test` and save them as `correct_classes`
- Execute the code to remove `crime` from `test` set

Hints:
- You can get the number of rows with `nrow(*name_of_the_dataframe*)`
- `train` and `test` are data frames, so you can access their columns with `$` mark

### R code
```{r}
# Work with the exercise in this chunk, step-by-step. Fix the R code!
# boston_scaled is available

# number of rows in the Boston dataset 
n <- nrow(boston_scaled)
n
# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- boston_scaled$crime
correct_classes
boston_scaled
table(boston_scaled$crime)
  # remove the crime variable from test data
test <- dplyr::select(test, -crime)

```


## 4.6 Linear discriminant analysis

[Linear discriminant analysis](https://en.wikipedia.org/wiki/Linear_discriminant_analysis) is a classification (and dimension reduction) method. It finds the (linear) combination of the variables that separate the target variable classes. The target can be binary or multiclass variable. 

Linear discriminant analysis is closely related to many other methods, such as principal component analysis (we will look into that next week) and the already familiar logistic regression. 

LDA can be visualized with a biplot. We will talk more about biplots next week. The LDA biplot arrow function used in the exercise is (with slight changes) taken from [this] (http://stackoverflow.com/questions/17232251/how-can-i-plot-a-biplot-for-lda-in-r) Stack Overflow message thread.

```{r, echo=FALSE}
# Pre-exercise-code (Run this code chunk first! Do NOT edit it.)

# Click the green arrow ("Run Current Chunk") in the upper-right corner of this chunk. This will initialize the R objects needed in the exercise. Then move to Instructions of the exercise to start working.

library(MASS)
boston_scaled <- read.table("https://raw.githubusercontent.com/KimmoVehkalahti/Helsinki-Open-Data-Science/master/datasets/boston_scaled.txt",
                            sep=",", header = T)
ind <- sample(nrow(boston_scaled),  size = nrow(boston_scaled) * 0.8)
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,]
correct_classes <- test$crime
test <- dplyr::select(test, -crime)
```

### Instructions
- Fit a linear discriminant analysis with the function `lda()`. The function takes a formula (like in regression) as a first argument. Use the `crime` as a target variable and all the other variables as predictors. Hint! You can type `target ~ .` where the dot means all other variables in the data.
- Print the `lda.fit` object
- Create a numeric vector of the train sets crime classes (for plotting purposes)
- Use the function `plot()` on the `lda.fit` model. The argument `dimen` can be used to choose how many discriminants is used. 
- Adjust the code: add arguments `col = classes` and `pch = classes` to the plot.
- Execute the `lda.arrow()` function (if you haven't done that already). Draw the plot with the lda arrows. Note that in DataCamp you will need to select both lines of code and execute them at the same time for the `lda.arrow()` function to work.
- You can change the `myscale` argument in `lda.arrow()` to see more clearly which way the arrows are pointing.

Hints:
- The formula looks like `target ~ .`
- The target variable in here is `crime`
- You can change a factor to numeric with `as.numeric()`
- Remember to execute the `lda.arrows()` code together with the plot in DataCamp. Otherwise the arrows won't work.

### R code
```{r}
# Work with the exercise in this chunk, step-by-step. Fix the R code!
# data train is available

# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train) 

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(factor(train$crime))
classes
classes <- train$crime

# plot the lda results
plot(lda.fit, dimen = 2,  pch = classes, col = classes)
lda.arrows(lda.fit, myscale = 4)

```


## 4.7 Predict LDA

Like in the regression, the function `predict()` can be used to predict values based on a model. The function arguments are almost the same. You can see the help page of prediction function for LDA with `?predict.lda`.

We split our data earlier so that we have the test set and the correct class labels. See how the LDA model performs when predicting on new (test) data.

```{r, echo=FALSE}
# Pre-exercise-code (Run this code chunk first! Do NOT edit it.)

# Click the green arrow ("Run Current Chunk") in the upper-right corner of this chunk. This will initialize the R objects needed in the exercise. Then move to Instructions of the exercise to start working.

library(MASS)
boston_scaled <- read.table("https://raw.githubusercontent.com/KimmoVehkalahti/Helsinki-Open-Data-Science/master/datasets/boston_scaled.txt",
                            sep=",", header = T)
ind <- sample(nrow(boston_scaled),  size = nrow(boston_scaled) * 0.8)
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,]
test
correct_classes <- test$crime
correct_classes
test <- dplyr::select(test, -crime)

lda.fit = lda(crime ~ ., data=train)
```

### Instructions
- Predict the crime classes with the `test` data. Like in regression, the `predict()` function takes the model object as a first argument.
- Create a table of the correct classes and the predicted ones. You can get the predicted classes with `lda.pred$class`. 
- Look at the table. Did the classifier predict the crime rates correctly?

Hints:
- `table(*object_name*)` creates a cross tabulation of the object
- You can get the predicted classes with `lda.pred$class`

### R code
```{r}
# Work with the exercise in this chunk, step-by-step. Fix the R code!
# lda.fit, correct_classes and test are available

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)
lda.pred
# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)

```


## 4.8 Towards clustering: distance measures

Similarity or dissimilarity of objects can be measured with distance measures. There are many different measures for different types of data. The most common or "normal" distance measure is [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance). 

There are functions that calculate the distances in R. In this exercise, we will be using the base R's `dist()` function. The function creates a distance matrix that is saved as dist object. The distance matrix is usually square matrix containing the pairwise distances of the observations. So with large datasets, the computation of distance matrix is time consuming and storing the matrix might take a lot of memory.

```{r, echo=FALSE}
# Pre-exercise-code (Run this code chunk first! Do NOT edit it.)

# Click the green arrow ("Run Current Chunk") in the upper-right corner of this chunk. This will initialize the R objects needed in the exercise. Then move to Instructions of the exercise to start working.

library(MASS)
data("Boston")
```

### Instructions
- Load the MASS package and the `Boston` dataset from it
- Create `dist_eu` by calling the `dist()` function on the Boston dataset. Note that by default, the function uses Euclidean distance measure.
- Look at the summary of the `dist_eu`
- Next create object `dist_man` that contains the Manhattan distance matrix of the Boston dataset
- Look at the summary of the `dist_man`

Hints:
- `data(*name_of_the_dataset*)` can be used to load dataset from R package
- See `?dist`. The argument you will need to change the distance measure is called `method`
- Remember that strings need the quotes in R

### R code
```{r}
# Work with the exercise in this chunk, step-by-step. Fix the R code!
# Boston dataset is available

# euclidean distance matrix
dist_eu <- dist(Boston)

# look at the summary of the distances
summary(dist_eu)
?dist()
# manhattan distance matrix
dist_man <- dist(Boston, method = "manhattan")
dist_man
# look at the summary of the distances
summary(dist_man)

```


## 4.9 K-means clustering

[K-means](https://en.wikipedia.org/wiki/K-means_clustering) is maybe the most used and known clustering method. It is an unsupervised method, that assigns observations to groups or **clusters** based on similarity of the objects. In the previous exercise we got a hang of **distances**. The `kmeans()` function counts the distance matrix automatically, but it is good to know the basics.  Let's cluster a bit!

```{r, echo=FALSE}
# Pre-exercise-code (Run this code chunk first! Do NOT edit it.)

# Click the green arrow ("Run Current Chunk") in the upper-right corner of this chunk. This will initialize the R objects needed in the exercise. Then move to Instructions of the exercise to start working.

library(MASS)
library(ggplot2)
data("Boston")
set.seed(13)
```

### Instructions

- First change the centers in the `kmeans()` function to be `4` and execute the clustering code
- Plot the Boston data with `pairs()`. Adjust the code by adding the `col` argument. Set the color based on the clusters that k-means produced. You can access the cluster numbers with `km$cluster`. What variables do seem to effect the clustering results? Note: With `pairs()` you can reduce the number of pairs to see the plots more clearly. On line 7, just replace `Boston` with for example `Boston[6:10]` to pair up 5 columns (columns 6 to 10).
- Try a different number of clusters: `1`, `2` and `3` (leave it to `3`). Visualize the results.

Hints:
- You can change the number of the cluster you want to have with the `centers` argument in `kmeans()`
- See `?kmeans`
- You can access the cluster numbers with `km$cluster`

### R code
```{r}
# Work with the exercise in this chunk, step-by-step. Fix the R code!
# Boston dataset is available
glimpse(Boston)
# k-means clustering
km <- kmeans(Boston, centers = 4)
km
# plot the Boston dataset with clusters
pairs(Boston[6:10], col = km$cluster)
pairs(Boston[1:5], col = km$cluster)
pairs(Boston[11:14], col = km$cluster)

?pairs
```


## 4.10 K-means: determine the k

K-means needs the number of clusters as an argument. There are many ways to look at the optimal number of clusters and a good way might depend on the data you have. 

One way to determine the number of clusters is to look at how the total of within cluster sum of squares (WCSS) behaves when the number of cluster changes (the calculation of total WCSS was explained in the video before). When you plot the number of clusters and the total WCSS, the optimal number of clusters is when the total WCSS drops radically.

K-means might produce different results every time, because it randomly assigns the initial cluster centers. The function `set.seed()` can be used to deal with that.

```{r, echo=FALSE}
# Pre-exercise-code (Run this code chunk first! Do NOT edit it.)

# Click the green arrow ("Run Current Chunk") in the upper-right corner of this chunk. This will initialize the R objects needed in the exercise. Then move to Instructions of the exercise to start working.

library(MASS)
library(ggplot2)
data("Boston")
```

### Instructions
- Set the max number of clusters (`k_max`) to be 10
- Execute the code to calculate total WCSS. This might take a while.
- Visualize the total WCSS when the number of cluster goes from 1 to 10. The optimal number of clusters is when the value of total WCSS changes radically. In this case, two clusters would seem optimal. 
- Run `kmeans()` again with two clusters and visualize the results

Hints:
- Simply adjust the part of the code that says "change me!"
- You can change the number of the cluster you want to have with the `centers` argument in `kmeans()`
- Numeric values do not need quotes in R

### R code
```{r, fig.height=8}
# Work with the exercise in this chunk, step-by-step. Fix the R code!
# MASS, ggplot2 and Boston dataset are available
set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

# k-means clustering
km <- kmeans(Boston, centers = 2)
km
# plot the Boston dataset with clusters
pairs(Boston[1:8], col = km$cluster)
pairs(Boston[9:14], col = km$cluster)
```

**Well done!**








































